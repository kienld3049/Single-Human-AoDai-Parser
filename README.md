# Segment Workflow

Workflow tÃ­ch há»£p DWPose, Grounded-SAM vÃ  cÃ¡c cÃ´ng cá»¥ xá»­ lÃ½ áº£nh Ä‘á»ƒ phÃ¢n Ä‘oáº¡n vÃ  phÃ¢n tÃ­ch hÃ¬nh áº£nh ngÆ°á»i. 

## ðŸ“‹ Má»¥c Lá»¥c

- [Tá»•ng Quan](#tá»•ng-quan)
- [YÃªu Cáº§u Há»‡ Thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Äáº·t](#cÃ i-Ä‘áº·t)
- [Táº£i Checkpoints](#táº£i-checkpoints)
- [Cáº¥u TrÃºc ThÆ° Má»¥c](#cáº¥u-trÃºc-thÆ°-má»¥c)
- [HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng](#hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
- [Troubleshooting](#troubleshooting)

## ðŸŽ¯ Tá»•ng Quan

Project nÃ y cung cáº¥p workflow hoÃ n chá»‰nh Ä‘á»ƒ:
- **DWPose**: PhÃ¡t hiá»‡n tÆ° tháº¿ ngÆ°á»i (pose estimation)
- **Grounded-SAM**: PhÃ¢n Ä‘oáº¡n Ä‘á»‘i tÆ°á»£ng dá»±a trÃªn text prompt
- **NMS**: Non-Maximum Suppression Ä‘á»ƒ loáº¡i bá» bounding boxes trÃ¹ng láº·p
- **mIoU**: ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n Ä‘oáº¡n

## ðŸ’» YÃªu Cáº§u Há»‡ Thá»‘ng

- **Python**: >= 3.8
- **PyTorch**: >= 1.7
- **TorchVision**: >= 0.8
- **CUDA**: 11.3 hoáº·c cao hÆ¡n (khuyáº¿n nghá»‹ cho GPU)
- **RAM**: Tá»‘i thiá»ƒu 8GB
- **GPU**: NVIDIA GPU vá»›i CUDA support (khuyáº¿n nghá»‹)

## ðŸ”§ CÃ i Äáº·t

### BÆ°á»›c 1: Táº¡o Conda Environment

```bash
# Táº¡o environment tá»« file cáº¥u hÃ¬nh
conda env create -f environment.yaml

# KÃ­ch hoáº¡t environment
conda activate control-v11
```

### BÆ°á»›c 2: CÃ i Äáº·t DWPose Dependencies

```bash
# CÃ i Ä‘áº·t ONNX Runtime (CPU)
pip install onnxruntime

# Hoáº·c cÃ i Ä‘áº·t vá»›i GPU support (khuyáº¿n nghá»‹)
pip install onnxruntime-gpu
```

**LÆ°u Ã½**: Náº¿u gáº·p khÃ³ khÄƒn vá»›i onnxruntime, tham kháº£o [opencv_onnx branch](https://github.com/IDEA-Research/DWPose/tree/opencv_onnx). 

### BÆ°á»›c 3: CÃ i Äáº·t Grounded-SAM

#### Thiáº¿t Láº­p Biáº¿n MÃ´i TrÆ°á»ng (cho GPU)

```bash
export AM_I_DOCKER=False
export BUILD_WITH_CUDA=True
export CUDA_HOME=/usr/local/cuda-11.3/  # Äiá»u chá»‰nh theo Ä‘Æ°á»ng dáº«n CUDA cá»§a báº¡n
```

#### CÃ i Äáº·t Segment Anything

```bash
python -m pip install -e segment_anything
```

#### CÃ i Äáº·t Grounding DINO

```bash
pip install --no-build-isolation -e GroundingDINO
```

#### CÃ i Äáº·t Dependencies Bá»• Sung

```bash
# Diffusers
pip install --upgrade diffusers[torch]

# OpenCV vÃ  cÃ¡c thÆ° viá»‡n xá»­ lÃ½ áº£nh
pip install opencv-python pycocotools matplotlib onnxruntime onnx ipykernel
```

## ðŸ“¦ Táº£i Checkpoints

### DWPose Models

```bash
# Táº¡o thÆ° má»¥c checkpoints
mkdir -p annotator/ckpts

# Táº£i DWPose model
# dw-ll_ucoco_384.onnx: 
# - Baidu: https://pan. baidu.com/s/1nuBjw-KKSxD_BkpmwXUJiw?pwd=28d7
# - Google Drive: https://drive.google.com/file/d/12L8E2oAgZy4VACGSK9RaZBZrfgx7VTA2/view?usp=sharing

# Táº£i Detection model
# yolox_l.onnx:
# - Baidu: https://pan.baidu.com/s/1fpfIVpv5ypo4c1bUlzkMYQ?pwd=mjdn
# - Google Drive: https://drive.google.com/file/d/1w9pXC8tT0p9ndMN-CArp1__b2GbzewWI/view?usp=sharing
```

### Grounded-SAM Models

```bash
# Táº£i SAM checkpoint (1.2GB)
wget https://dl. fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

# Táº£i GroundingDINO checkpoint (694MB)
wget https://github. com/IDEA-Research/GroundingDINO/releases/download/v0.1. 0-alpha2/groundingdino_swinb_cogcoor.pth
```

## ðŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
Segment-workflow-main/
â”œâ”€â”€ annotator/                    # CÃ¡c annotator modules
â”‚   â”œâ”€â”€ ckpts/                   # Checkpoints cho DWPose
â”‚   â”‚   â”œâ”€â”€ dw-ll_ucoco_384.onnx
â”‚   â”‚   â””â”€â”€ yolox_l.onnx
â”‚   â”œâ”€â”€ dwpose/                  # DWPose implementation
â”‚   â””â”€â”€ ... 
â”œâ”€â”€ GroundingDINO/               # GroundingDINO module
â”œâ”€â”€ segment_anything/            # Segment Anything Model
â”œâ”€â”€ get_dwpose_results.py        # Script cháº¡y DWPose
â”œâ”€â”€ get_grounded_sam_output.py   # Script cháº¡y Grounded-SAM
â”œâ”€â”€ get_miou.py                  # Script tÃ­nh mIoU
â”œâ”€â”€ non_max_suppression.py       # Script NMS
â”œâ”€â”€ environment.yaml             # Conda environment config
â””â”€â”€ README.md
```

## ðŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. Cháº¡y DWPose (Pose Estimation)

```bash
python get_dwpose_results.py \
    --input_dir ./input_images \
    --output_dir ./output_dwpose
```

**Parameters**:
- `--input_dir`: ThÆ° má»¥c chá»©a áº£nh Ä‘áº§u vÃ o
- `--output_dir`: ThÆ° má»¥c lÆ°u káº¿t quáº£ pose estimation

### 2. Cháº¡y Grounded-SAM (Segmentation)

```bash
python get_grounded_sam_output. py \
    --config GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py \
    --grounded_checkpoint groundingdino_swinb_cogcoor.pth \
    --sam_checkpoint sam_vit_h_4b8939.pth \
    --input_image_dir ./input_images \
    --output_dir ./output_segments \
    --box_threshold 0.3 \
    --text_threshold 0.25 \
    --dataset "ATR"
```

**Parameters**:
- `--config`: File cáº¥u hÃ¬nh GroundingDINO
- `--grounded_checkpoint`: GroundingDINO model checkpoint
- `--sam_checkpoint`: SAM model checkpoint
- `--input_image_dir`: ThÆ° má»¥c áº£nh Ä‘áº§u vÃ o
- `--output_dir`: ThÆ° má»¥c lÆ°u káº¿t quáº£
- `--box_threshold`: NgÆ°á»¡ng confidence cho bounding box (0.0-1.0)
- `--text_threshold`: NgÆ°á»¡ng confidence cho text matching (0.0-1.0)
- `--dataset`: Dataset format ("ATR", "COCO", etc.)

### 3. Cháº¡y Non-Maximum Suppression

```bash
python non_max_suppression.py \
    --input_dir ./detections \
    --output_dir ./nms_results \
    --iou_threshold 0.5
```

**Parameters**:
- `--input_dir`: ThÆ° má»¥c chá»©a detection results
- `--output_dir`: ThÆ° má»¥c lÆ°u káº¿t quáº£ sau NMS
- `--iou_threshold`: IoU threshold cho NMS (default: 0.5)

### 4. TÃ­nh mIoU (Evaluation)

```bash
python get_miou.py \
    --pred_dir ./predictions \
    --gt_dir ./ground_truth \
    --num_classes 18
```

**Parameters**:
- `--pred_dir`: ThÆ° má»¥c chá»©a predicted masks
- `--gt_dir`: ThÆ° má»¥c chá»©a ground truth masks
- `--num_classes`: Sá»‘ lÆ°á»£ng classes (ATR: 18, LIP: 20)

## ðŸŽ¨ VÃ­ Dá»¥ Workflow HoÃ n Chá»‰nh

```bash
# 1. KÃ­ch hoáº¡t environment
conda activate control-v11

# 2.  Táº¡o thÆ° má»¥c output
mkdir -p outputs/{dwpose,segments,nms,evaluation}

# 3. Cháº¡y DWPose
python get_dwpose_results.py \
    --input_dir ./data/images \
    --output_dir ./outputs/dwpose

# 4. Cháº¡y Grounded-SAM
python get_grounded_sam_output.py \
    --config GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py \
    --grounded_checkpoint groundingdino_swinb_cogcoor.pth \
    --sam_checkpoint sam_vit_h_4b8939.pth \
    --input_image_dir ./data/images \
    --output_dir ./outputs/segments \
    --box_threshold 0.3 \
    --text_threshold 0.25 \
    --dataset "ATR"

# 5. Ãp dá»¥ng NMS (náº¿u cáº§n)
python non_max_suppression.py \
    --input_dir ./outputs/segments \
    --output_dir ./outputs/nms \
    --iou_threshold 0.5

# 6. ÄÃ¡nh giÃ¡ vá»›i mIoU (náº¿u cÃ³ ground truth)
python get_miou.py \
    --pred_dir ./outputs/segments \
    --gt_dir ./data/ground_truth \
    --num_classes 18
```

## ðŸ” Troubleshooting

### Lá»—i CUDA

**Váº¥n Ä‘á»**: `RuntimeError: CUDA out of memory`

**Giáº£i phÃ¡p**:
```bash
# Giáº£m batch size hoáº·c image resolution
# Hoáº·c dÃ¹ng CPU mode
export CUDA_VISIBLE_DEVICES=""
```

### Lá»—i Import

**Váº¥n Ä‘á»**: `ModuleNotFoundError: No module named 'groundingdino'`

**Giáº£i phÃ¡p**:
```bash
# CÃ i Ä‘áº·t láº¡i GroundingDINO
pip install --no-build-isolation -e GroundingDINO
```

### Lá»—i ONNX Runtime

**Váº¥n Ä‘á»**: ONNX Runtime khÃ´ng tÆ°Æ¡ng thÃ­ch

**Giáº£i phÃ¡p**:
```bash
# Gá»¡ cÃ i Ä‘áº·t vÃ  cÃ i láº¡i
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu  # hoáº·c onnxruntime cho CPU
```

### Checkpoints KhÃ´ng TÃ¬m Tháº¥y

**Váº¥n Ä‘á»**: `FileNotFoundError: [Errno 2] No such file or directory: 'xxx.pth'`

**Giáº£i phÃ¡p**:
- Kiá»ƒm tra checkpoints Ä‘Ã£ Ä‘Æ°á»£c táº£i vá» Ä‘Ãºng thÆ° má»¥c
- Äáº£m báº£o Ä‘Æ°á»ng dáº«n trong command chÃ­nh xÃ¡c
- Sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i náº¿u cáº§n

## ðŸ“š TÃ i Liá»‡u Tham Kháº£o

- [Segment Anything](https://github.com/facebookresearch/segment-anything)
- [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)
- [DWPose](https://github. com/IDEA-Research/DWPose)
- [ControlNet](https://github.com/lllyasviel/ControlNet)

## ðŸ“ Notes

- Äáº£m báº£o cÃ³ Ä‘á»§ dung lÆ°á»£ng á»• cá»©ng (~5GB cho checkpoints)
- GPU memory tá»‘i thiá»ƒu 8GB cho cÃ¡c models lá»›n
- Thá»i gian xá»­ lÃ½ phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c áº£nh vÃ  hardware

## âš–ï¸ License

Tham kháº£o LICENSE files trong cÃ¡c thÆ° má»¥c con cho thÃ´ng tin chi tiáº¿t.

# For DWPose (Legacy Documentation)
ðŸŒµðŸŒµðŸŒµ This environment helps you to apply DWPose to ControlNet and prepare for installing Grounded-SAM.

ðŸŒµ First, make sure to run ControlNet successfully.
```
# Set ControlNet environment
conda env create -f environment.yaml
conda activate control-v11
```
ðŸŒµ Second, install tools to apply DWPose to ControlNet. If it's hard to install onnxruntime, you can refer branch [opencv_onnx](https://github.com/IDEA-Research/DWPose/tree/opencv_onnx), which runs the onnx model with opencv.
```
# Set ControlNet environment
pip install onnxruntime
# if gpu is available
pip install onnxruntime-gpu
```

# Grounded-Segment-Anything (Legacy Documentation)

## Installation
The code requires `python>=3.8`, as well as `pytorch>=1.7` and `torchvision>=0.8`. Please follow the instructions [here](https://pytorch.org/get-started/locally/) to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.

### Install without Docker (Recommended)
You should set the environment variable manually as follows if you want to build a local GPU environment for Grounded-SAM:
```bash
export AM_I_DOCKER=False
export BUILD_WITH_CUDA=True
export CUDA_HOME=/path/to/cuda-11.3/
```

Install Segment Anything:

```bash
python -m pip install -e segment_anything
```

Install Grounding DINO:

```bash
pip install --no-build-isolation -e GroundingDINO
```


Install diffusers:

```bash
pip install --upgrade diffusers[torch]
```

The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. `jupyter` is also required to run the example notebooks.

```
pip install opencv-python pycocotools matplotlib onnxruntime onnx ipykernel
```

More details can be found in [install segment anything](https://github.com/facebookresearch/segment-anything#installation) and [install GroundingDINO](https://github.com/IDEA-Research/GroundingDINO#install) and [install OSX](https://github.com/IDEA-Research/OSX)

# How to get segment images

First, you need to create your image folder and download all necessary checkpoints for models:

For DWPose: Download dw-ll_ucoco_384.onnx ([baidu](https://pan.baidu.com/s/1nuBjw-KKSxD_BkpmwXUJiw?pwd=28d7), [google](https://drive.google.com/file/d/12L8E2oAgZy4VACGSK9RaZBZrfgx7VTA2/view?usp=sharing)) and Det model yolox_l.onnx ([baidu](https://pan.baidu.com/s/1fpfIVpv5ypo4c1bUlzkMYQ?pwd=mjdn), [google](https://drive.google.com/file/d/1w9pXC8tT0p9ndMN-CArp1__b2GbzewWI/view?usp=sharing)), then put them into annotator/ckpts.

For GroundingDINO: Download SwinB checkpoints for the best quality ([Github link](https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth)).

For Segment Anything Model: Download the SAM-ViT-h checkpoints ([link](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth)). 

Or run this command to get both checkpoints for Grounded-SAM:

```

wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth
```

Then, simply run:

```

python get_grounded_sam_output.py   --config GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py   --grounded_checkpoint groundingdino_swinB_cogcoor.pth   --sam_checkpoint sam_vit_h_4b8939.pth   --input_image_dir [YOUR_IMAGE_FOLDER]   --output_dir [YOUR_OUTPUT_FOLDER]   --box_threshold 0.3   --text_threshold 0.25 --dataset "ATR"
```
